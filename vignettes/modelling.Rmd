---
title: "Modelling and feature selection"
subtitle: "`r paste0('metabolyseR v',packageVersion('metabolyseR'))`"
author: "Jasen Finch"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  prettydoc::html_pretty:
    toc: true
    highlight: github
    theme: tactile
vignette: >
  %\VignetteIndexEntry{Modelling and feature selection}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(knitr)
library(dplyr)
library(purrr)
library(stringr)

opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = 'center'
)
```

## Introduction

Modelling provides the essential data mining step for extracting biological information and explanatory metabolome features from a data set relating to the experimental conditions.
`metabolyseR` provides a number of both univariate and multivariate methods for data mining.

For an introduction to the usage of *metabolyseR* for both exploratory and routine analyses, see the introduction vignette using:

```{r introduction-vignette,eval=FALSE}
vignette('introduction','metabolyseR')
```

To further supplement this document, a quick start example analysis is also available as a vignette:

```{r example-analysis,eval=FALSE}
vignette('quick_start','metabolyseR')
```

To begin, the package can be loaded using:

```{r package-load}
library(metabolyseR)
```

### Example data

The examples used here will use the `abr1` data set from the [metaboData](https://aberhrml.github.io/metaboData/) package. 
This is nominal mass flow-injection mass spectrometry (FI-MS) fingerprinting data from a plant-pathogen infection time course experiment.
The pipe `%>%` from the [magrittr](https://magrittr.tidyverse.org/) package will also be used.
The example data can be loaded using:

```{r data-load}
library(metaboData)
```

Only the negative acquisition mode data (`abr1$neg`) will be used along with the sample meta-information (`abr1$fact`).
Create an `AnalysisData` class object, assigned to the variable `d`, using the following:

```{r example-data}
d <- analysisData(abr1$neg[,1:500],abr1$fact)
```

```{r print-data}
print(d)
```

As can be seen above the data set contains a total of `r nSamples(d)` samples and `r nFeatures(d)` features.

## Random Forest

Random forest is a versatile ensemble machine learning approach based on forests of decision trees for multivariate data mining.
This can include **unsupervised** analysis, **classification** of discrete response variables and **regression** of continuous responses.

Random forest can be performed in `metabolyseR` using the `randomForest` function.
For further details on the arguments for using this function, see `?randomForest`.
This implementation of random forest in `metabolyseR` utilises the `randomForest` package.
See `?randomForest::randomForest` for more information about that implementation.

### Unsupervised

The unsupervised random forest approach can be useful starting point for analysis in any experimental context.
It can be used to give a general overview of the structure of the data and to identify an possible problems such as the presence of outliers samples or splits in the data caused by the impact of analytical or sample preparation factors. 
Unsupervised random forest can have advantages in these assessments over other approaches such as Principle Component Analysis (PCA) as it is less sensitive to the effect of a single feature that in fact could have little overall impact relative to the other hundreds that could be present in a data set.

The examples below will show the use of unsupervised random forest for assessing the general structure of the example data set and the presence of outlier samples.

Unsupervised random forest can be performed by setting the `cls` argument to `NULL`:

```{r unsupervised}
unsupervised_rf <- d %>%
  randomForest(cls = NULL)
```

The type of random forest that has been performed can be checked using the `type` method.

```{r rf-type}
type(unsupervised_rf)
```

To assess the resulting model, the proximity of the observations can be visualised using multidimensional scaling (MDS) as shown below.

Firstly, the presence of outlier samples will be assessed.
A single class response factor named `single` will first be added to the sample information to aid visualisation.

```{r add-single-class}
unsupervised_rf <- clsAdd(unsupervised_rf,
                          'single',
                          rep(1,nSamples(unsupervised_rf)))
```

The MDS plot visualised using the following, specifying the `single` class information column by which to colour the observation points.
The individual points are also labelled by their injection order to enable the identification of individual samples if necessary.

```{r outlier-detect}
plotMDS(unsupervised_rf,
        cls = 'single',
        label = 'injorder',
        labelSize = 3,
        title = 'Outlier detection')
```

From the plot above, it can be seen that it is unlikely that any of the samples can be considered as outliers as all the samples are found within the 95% confidence ellipse.

The structure of these observations can be investigated further by colouring the points by a different experimental factor.
This will be by the `day` class column which is the main experimental factor of interest in this experiment. 

```{r unsupervised-rf}
plotMDS(unsupervised_rf,
        cls = 'day')
```

This shows that it is indeed the experimental factor of interest that is having the greatest impact on the structure of the data.
The progression of the experimental time points are obvious across Dimension 1.

```{r importance-metrics}
importanceMetrics(unsupervised_rf)
```

```{r}
importance(unsupervised_rf)
```

```{r}
unsupervised_rf %>%
  explanatoryFeatures()
```

```{r}
unsupervised_rf %>%
  plotFeature(feature = 'N153',
              cls = 'day')
```

### Classification

Random forest classification assesses the extent of discrimination (difference) between classes of a discrete response variable. 
This includes both multinomial (number of classes > 2) and binary (number of classes = 2) comparisons.

In multinomial situations, the suitability of a multinomial comparison versus multiple binary comparisons can depend on the experimental context.
For instance, in a treatment/control experiment that includes multiple time points, a multinomial comparison using all available classes could be useful to visualise the general structure of the data. 
However, it could make any extracted explanatory features difficult to reason about as to how they relate to the individual experimental time point or treatment conditions.
An investigator could then identify the relevant binary comparisons to the biological question and focus the further classification comparisons to better select for explanatory features.

#### Multinomial comparisons

In situations 
Multinomial classification is for situations where more than 

Multinomial classification can be performed on the `day` response variable of the example data set using the following:

```{r classification}
multinomial_rf <- d %>%
  randomForest(cls = 'day',
               nCores = 2)
```

To assess the model performance we can extract a table of the performance metrics:

```{r classification-metrics }
multinomial_rf %>%
  metrics()
```

These metrics include **accuracy**, **Cohen's kappa** (kap), **area under the receiver operator characteristic curve** (roc_auc) and **margin**.
Each metric has both strengths and weaknesses depending on the context of the classification such as an imbalance of observations between the classes.

```{r class-frequencies}
d %>% 
  clsExtract(cls = 'day') %>% 
  table()
```

```{r classification-mds}
multinomial_rf %>% 
  plotMDS(cls = 'day')
```

```{r classification-roc}
multinomial_rf %>% 
  plotROC()
```

```{r classification-importance}
multinomial_rf %>%
  importance()
```

```{r classification-importance-metrics}
importanceMetrics(multinomial_rf)
```

```{r classification-explanatory}
multinomial_rf %>%
  explanatoryFeatures(metric = 'MeanDecreaseGini',
                      threshold = 0.1)
```

```{r classification-multiple-responses}
d %>%
  randomForest(cls = c('name','day'),nCores = 2) %>%
  metrics()
```

```{r classification-multiple-mds}
d %>%
  randomForest(cls = c('name','day'),nCores = 2) %>%
  plotMDS()
```

```{r classification-classes}
d %>% 
  clsExtract(cls = 'day') %>% 
  unique()
```

```{r classification-comparison}
d %>%
  randomForest(cls = 'day',
               comparisons = list(day = '1~2~3'),
               nCores = 2) %>%
  metrics()
```

```{r}
d %>%
  keepClasses(cls = 'day',
              classes = c('1','2','3')) %>%
  randomForest(cls = 'day',
               binary = TRUE) %>%
  plotMetrics()
```

#### Binary comparisons

```{r}
binaryComparisons(d,cls = 'day')
```

```{r}
binary_comparisons <- binaryComparisons(d,cls = 'day') %>% 
  .[stringr::str_detect(.,'H')]
```


```{r}
binary_rf <- d %>%
  randomForest(cls = 'day',
               comparisons = list(day = binary_comparisons))
```

```{r}
binary_rf %>% 
  plotMetrics()
```

```{r}
binary_rf %>% 
  explanatoryFeatures()
```
```{r}
refactor_cls <- clsExtract(binary_rf,
                           cls = 'day') %>% 
  factor(.,levels = c('H','1','2','3','4','5'))

binary_rf <- clsReplace(binary_rf,
                        value = refactor_cls,
                        cls = 'day')
binary_rf %>% 
  plotExplanatoryHeatmap()
```

### Regression

## Univariate analyses

## Routine analyses

The default parameters for modelling using Random Forest are shown below.

```{r parameters}
p <- analysisParameters(c('pre-treatment','modelling'))
```

```{r pre-treatment_parameters}
parameters(p,'pre-treatment') <- preTreatmentParameters(
  list(
    keep = 'classes',
    occupancyFilter = 'maximum',
    transform = 'TICnorm' 
  )
)
```

Parameters for alternative methods or multiple methods can be retrieved using the `modellingParameters()` function as shown below.

```{r changeModellingParameters}
parameters(p,'modelling') <- modellingParameters(c('randomForest','ttest'))
changeParameter(p,'cls') <- 'day'
changeParameter(p,'classes') <- c('H','1')
```

```{r print_parameters}
p
```

Descriptions of each of the parameters can be found in the documentation for a particular method (see table below). 

Modelling analysis can then be performed on the pre-treated data from the previous section.

```{r classificationAnalysis}
analysis <- metabolyse(abr1$neg,abr1$fact,p)
```
