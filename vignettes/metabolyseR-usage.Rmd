---
title: "metabolyseR"
author: "Jasen Finch"
date: "July 2017"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{metabolyseR-usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r libraryLoad,echo=FALSE}
suppressPackageStartupMessages(library(metabolyseR))
```

## Introduction

Extracting biological information  related to experimental treatments from metabolomics data involves a number of steps; data pre-treatment, assessing the extent discrimination between treatment classes and identifying explanatory features responsible for this discrimination.
The *metabolyseR* package provides a suite of methods that encompass these aspects of metabolomics data analysis.
Below shows the four analysis elements in that make up metabolyseR.

```{r metabolyseRDiagram,echo=FALSE,fig.width=6}
  DiagrammeR::grViz(readr::read_file('figures/metabolyseR.gv'))
```

This document will provide a tutorial on the basic usage of the package, followed by more detailed discussion of each of the analysis elements.
Lastly, there will be discussion of the tools for the visualisation of the results from these data as well as the integration of metabolyseR into wider metabolomics workflows.
This allows more bespoke analyses to be performed based on the particular metabolomics technique being used.

*metabolyseR* has been written to be as generic and versatile as possible to allow analyses to be tailored to the particular needs of the metabolomic technique being used.
With this in mind, it's application doesn't need to be restricted only to metabolomics data.
These analysis elements could, for example, also be suitable for data such as gene expression data where replication allows.

The examples shown here use the `abr1` dataset from the *[FIEmspro](https://github.com/wilsontom/FIEmspro)* package (`?FIEmspro::abr1`).

## Basic Usage

*metabolyseR* has three main functions for performing analyses:

* `analysisParameters` - allows the selection of analysis parameters.
* `metabolyse` - input data and info to analyse using the selected parameters.
* `reAnalysis` - re-analyse data previously analysed using `metabolyse` based on newly selected parameters.

### Parameter Selection
Parameter selection is the fundamental aspect of using *metabolyseR* and will be the most modified as it dictates how the analyses will be performed.
`analysisParameters` returns an object of class `AnalysisParameters` containing the relevant parameters of the selected analysis elements.
For example, the code below will return default parameters for all the *metabolyseR* analysis elements.

```{r parametersExample,eval=FALSE}
library(metabolyseR)
parameters <- analysisParameters()
parameters
```

To retrieve parameters for a subset of elements the following can be run, which will return parameters for only the `preTreat` and   `classification` elements. 

```{r parametersElementSubset,eval=FALSE}
parameters <- analysisParameters(c('preTreat','classification'))
parameters
```

The parameters for the individual analysis elements in the `AnalysisParameters` object can be accessed using the `@` extraction operator.
For instance following code illustrates how to modify the occupancy threshold parameter for class imputation for data pre-treatment from 2/3 to 1/2.

```{r parametersSetExample,eval=FALSE}
parameters@preTreat$impute$class$occupancy <- 1/2
parameters
```

Discussion of parameters particular to the individual analysis elements can be found in the relevant analysis element sections.

### Analysis

Executing analyses is simple using the `metabolyse` function with specification of the data to use, the sample meta-information (info) and the analysis parameters.
This is shown by the following code.

```{r metabolyseExample,eval=FALSE}
data(abr1,package = 'FIEmspro')
parameters <- analysisParameters(c('preTreat','classification'))
parameters@preTreat <- list(
    occupancyFilter = list(maximum = list()),
    transform = list(TICnorm = list())
)
analysis <- metabolyse(abr1$neg,abr1$fact,parameters) 
analysis
```

The data and info can be either `tibble` or `data.frame` structures and the parameters should be an S4 object of class `AnalysisParameters`.
The order of the rows of the info table should agree with those in the data table.

`metabolyse` returns an S4 object of class `Analysis`.
Data and analysis results can be extracted from this object using the following functions:

* `rawData` - raw data and info.
* `preTreatedData` - pre-treated data and info.
* `classificationResults` - classification results.
* `featureSelectionResults` - feature selection results.
* `correlationResults` - correlation results.

If data pre-treatment has not been performed prior to classification or feature selection, the raw data will automatically be used.
The date and time of each of analysis of each of the elements is also stored and can is displayed when the object is printed.

### Re-Analysis

There are likely to be occasions where analyses need to be re-analysed using a new set of parameters. 
`reAnalyse` can be used for this.
It simply requires the specification of the `Analysis` object and parameters for the re-analysis as shown below.

```{r reAnalyseExample,eval=FALSE}
parameters <- analysisParameters('featureSelection')
analysis <- reAnalyse(analysis,parameters)
analysis
```

## Pre-Treatment

Data pre-treatment prior to statistical analyses is an essential step in the analysis of metabolomics data as it ensures the quality and integrity.
It is also important that appropriate pre-treatment strategies are used not only for the analytical technique being applied but are also suitable for the statistical or machine learning analyses that are to be used on the data.

Data pre-treatment is the most faceted aspect of the analysis elements in *metabolyseR*.
It itself is made up of a number of elements, which themselves are made up of methods.
The pre-treatment elements can be seen below.

```{r preTreatmentDiagram,echo=FALSE,fig.width=6}
  DiagrammeR::grViz(readr::read_file('figures/preTreat.gv'))
```

The parameter selection for the pre-treatment elements will firstly be discussed, with more in-depth discussion on the methods of the individual elements to follow. 

### Parameter Selection

The default pre-treatment parameters can be seen below.

```{r preTreatDefaults}
analysisParameters('preTreat')
```

This pre-treatment analysis is made up of three elements that include variable filtering based on QC samples, missing data imputation and data transformation (`QC`, `impute` and `transform` respectively).
Each of these elements is made up of varying numbers of methods, with each method having its own arguments.
The order in which these elements and their methods is displayed is the order in which their execution upon the data will take place.
This allows the fine specification of the data pre-treatment routines that are required for the particular data.

Pre-treatment routines can be customised by altering the `preTreat` slot within the `AnalysisParameters` object as shown below.

```{r preTreatParameterExample}
data("abr1",package = 'FIEmspro')
parameters <- analysisParameters('preTreat')
parameters@preTreat <- list(
    remove = list(
      class = list(
        cls = 'class',
        classes = unique(abr1$fact$class)[!(unique(abr1$fact$class) %in% c(1:2))])),
    occupancyFilter = list(
      maximum = list()),
    transform = list(
      TICnorm = list())
)
parameters
```

This routine will firstly remove a number of classes, filter the variables based on class occupancy and then transform the data using a total ion count normalisation.

The pre-treatment routines are specified as a hierarchy of lists within lists.
Firstly the pre-treatment elements should be specified within a list with the particular element methods specified as lists within these.
The method arguments are then declared within these lists.
Empty lists for methods can be specified to use the default arguments; however, element lists should not be empty. 

The pre-treatment routine can then be executed using the following.
```{r preTreatExample}
analysis <- metabolyse(abr1$neg,abr1$fact,parameters)
analysis
```

### Sample removal

The removal of samples prior to their analysis is important if there are outlier samples included or sample classes that are no longer relevant for the intended analyses.

The methods and their arguments for the removal of samples is shown in the table below.

```{r removeMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::removeMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::removeMethods())
defaults <- lapply(metabolyseR:::removeMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Variable filtering based on occupancy

Occupancy provides a useful measure by which to filter variables that are poorly represented (contain a majority of missing values).
An occupancy threshold provides a means of specifying this majority with variables below the threshold excluded from further analyses.
However, this can be complicated by an underlying class structure present within the data where a variable may be well represented within one class but not in another.

An occupancy threshold can be applied both minimally and maximally within the class structure.
A minimally applied threshold would require all the classes to have an occupancy above the threshold for the variable to be retained.
A maximally applied threshold would require only a single class to have an occupancy above the threshold for the variable to be retained.
The later strategy has the important benefit of retaining any presence and absence trends that may be present.

The methods and their arguments for occupancy based variable filtering are shown in the table below.

```{r occupancyMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::occupancyMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::occupancyMethods())
defaults <- lapply(metabolyseR:::occupancyMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Variable filtering based on QC samples

A QC sample is an average pooled sample, equally representative in composition of all the samples present within an experimental set.
Within an analytical run, the QC sample is analysed intermittently at equal intervals throughout the run.
If there is class structure within the run this should be randomised within a block fashion so that the classes are equally represented in each block throughout the run.
QC samples can then be injected between these randomised blocks.
This provides a set of technical injections that allows the variability in instrument performance over the run to be accounted for and the robustness of the acquired variables to be assessed.

The technical reproducibility of an acquired variable can be assessed using it's relative standard deviation (RSD) within the QC samples.
The variable RSDs can then be thresholded to filter out variables that are poorly reproducible across the analytical runs.
This variable filtering strategy has an advantage over that of occupancy alone as it is not dependent on underlying class structure.
Therefore the variables and variable numbers will not alter if a new class structure is imposed upon the data.

The methods and arguments for variable filtering based upon QC samples are shown in the table below.

```{r QCMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::QCMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::QCMethods())
defaults <- lapply(metabolyseR:::QCMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Imputation of missing data

Missing values can have an important influence on downstream analyses with zero values heavily influencing the outcomes of parametric tests.
Where and how they are imputed are important considerations and is highly related to variable occupancy.
Imputation accuracy is likely to be reduced if data is sparse or there is underlying class structure where there is significant discrimination.

The methods provided here allow both these aspects to be taken into account and utilise Random Forest imputation using the [*missForest*]( https://CRAN.R-project.org/package=missForest) package.
Their arguments are shown in the table below.

```{r imputeMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::imputeMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::imputeMethods())
defaults <- lapply(metabolyseR:::imputeMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Data transformation

Prior to downstream analyses, metabolomics data often require a transformation step to allow the data to meet the assumptions of a particular statistical inference or account for potential variability in sample concentration.

There are a wide range of transformation methods available that are commonly used for the analysis of metabolomics data.
The methods and their arguments are shown in the table below.

```{r transformMethods,echo=FALSE,results='asis'}
desc <- metabolyseR:::transformMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
if (T %in% sapply(args,function(x){grepl("''",x)})) {
  args[sapply(args,function(x){grepl("''",x)})] <- lapply(args[sapply(args,function(x){grepl("''",x)})],function(x){''})
}
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::transformMethods())
defaults <- lapply(metabolyseR:::transformMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
if (T %in% sapply(defaults,is.null)) {
  defaults[sapply(defaults,is.null)] <- lapply(defaults[sapply(defaults,is.null)],function(x){c(`''` = '')})
}
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})

if (T %in% sapply(defaults,function(x){grepl("''",x)})) {
  defaults[sapply(defaults,function(x){grepl("''",x)})] <- lapply(defaults[sapply(defaults,function(x){grepl("''",x)})],function(x){''})
}

if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

## Classification

Classification assesses the extent of discrimination between sample classes.
This analysis element utilises the `accest` function from the *[FIEmspro](https://github.com/wilsontom/FIEmspro)* package.
It uses re-sampling based methods on a pairwise comparison basis where the data are partitioned into training and test sets.
The training set is used to build the initial model based on a specified algorithm which is then used to predict the class labels of the test set.
This allows the assessment of a models accuracy, AUC and margin.
The re-sampling process is repeated numerous times allowing mean aggregation of these classification measures.
These methods use are executed using binary class comparisons all combinations of comparisons are run separately.

The default parameters for classification are shown below.

```{r classificationParameters}
parameters <- analysisParameters('classification')
parameters
```

These parameters denote the following:

* `cls` - The info column to use for class labels.
* `method` - The classifier to use (`randomForest`, `svm`, `nlda`). More than one can be selected at a time.
* `pars` - Classification parameters to be passed to `valipars` (`?FIEmspro::valipars`).
* `nCores` - The number of cores to use for parallelisation.
* `clusterType` - The cluster type to use for parallelisation (`?parallel::makeCluster`).

This classification analysis can then be performed on the pre-treated data from the previous section.

```{r classificationAnalysis,eval=FALSE}
analysis <- reAnalyse(analysis,parameters)
```

The classification results will then be returned as a table containing the accuracies, AUCs and margins for each iteration, method and pairwise.

## Feature Selection

Where discrimination exists between sample classes it is important to identify the variables (or features) that are responsible for this.
Feature selection methods provides a means of ranking variables relative to their contribution to the class discrimination and whether this is significant.
Explanatory features can then be targeted for further analyses such as annotation which can allow interpretation of their trends in a biological context.

The default parameters for feature selection are shown below.

```{r featureSelectionParameters}
parameters <- analysisParameters('featureSelection')
parameters
```

These parameters denote the following:

* `method` - The feature selection method to use (see table below). More than one can be selected at a time.
* `cls` - The info column to use for class labels.
* `pars` - Parameters for each feature selection method given as a named list. See table below.
* `nCores` - The number of cores to use for parallelisation.
* `clusterType` - The cluster type to use for parallelisation (`?parallel::makeCluster`).

This feature selection analysis can then be performed using the following.

```{r featureSelectionExample,eval=FALSE}
analysis <- reAnalyse(analysis,parameters)
```

The results will be returned as a table containing the scores for each variable,pairwise and method.

There are a number of methods available which are shown in the table below.

```{r fsMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::fsMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::fsMethods())
defaults <- lapply(metabolyseR:::fsMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

## Correlations

Correlations are important for finding associations between metabolomic features.
They can be indicative of isotopic, adduct and biotransformation relationships that can aid their annotation.

The default parameters for correlation analysis are shown below.

```{r correlationsParameters}
parameters <- analysisParameters('correlations')
parameters
```

These parameters denote the following:

* `method` - Correlation type (`?Hmisc::rcorr`).
* `pAdjustMethod` - Method for multiple testing p value correction (`?p.adjust`).
* `corPvalue` - Threshold for *p* value significance.
* `nCores` - The number of cores to use for parallelisation.
* `clusterType` - The cluster type to use for parallelisation (`?parallel::makeCluster`).

This correlation analysis can then be performed using the following.

```{r correlationsExample,eval=FALSE}
analysis <- reAnalyse(analysis,parameters)
```

The results will be returned as a table containing the significantly correlated bins, their correlation coefficients and their log~2~ intensity ratios.

## Visualisation and Workflow Integration

The [*metaboVis*](https://github.com/jasenfinch/metaboVis) package provides a collection of *shiny* applications for interactive visualisation of these analyses.

The generic nature of *metabolyseR* means that it can easily be integrated into workflows for the analysis of any metabolomics data.
The package [*metaboWorkflows*](https://github.com/jasenfinch/metaboWorkflows) provides wrapper methods for signal processing, analysis and annotation of metabolomics data.
The *metabolyseR* analysis elements are incorporated into the analysis aspect of this package.