---
title: "metabolyseR"
subtitle: "`r paste0('v',packageVersion('metabolyseR'))`"
author: "Jasen Finch"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  prettydoc::html_pretty:
    toc: true
    highlight: github
    theme: tactile
vignette: >
  %\VignetteIndexEntry{metabolyseR-usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r,include=FALSE}
knitr::opts_chunk$set(fig.align = 'center')
```

## Introduction

Extracting biological information  related to experimental treatments from metabolomics data involves a number of steps; data pre-treatment, assessing the extent of discrimination or association between treatment classes and identifying explanatory features responsible for this.
The *metabolyseR* package provides a suite of methods that encompass these aspects of metabolomics data analysis.
Below shows the four analysis elements in that make up metabolyseR.

<center>
```{r metabolyseRDiagram,echo=FALSE,fig.width=8}
  DiagrammeR::grViz(readr::read_file('figures/metabolyseR.gv'))
```
</center>

This document will provide a tutorial on the basic usage of the package, followed by more detailed discussion of each of the analysis elements.
Lastly, there will be discussion of the tools for the visualisation of the results from these data as well as the integration of metabolyseR into wider metabolomics workflows.
This allows more bespoke analyses to be performed based on the particular metabolomics technique being used.

*metabolyseR* has been written to be as generic and versatile as possible to allow analyses to be tailored to the particular needs of the metabolomic technique.
With this in mind, it's application doesn't need to be restricted only to metabolomics data.
These analysis elements could, for example, also be suitable for data such as gene expression data where replication allows.

The examples shown here use the `abr1` data set from the *[metaboData](https://github.com/aberHRML/metaboData)* package (`?metaboData::abr1`).

```{r libraryLoad,message=FALSE}
library(metabolyseR)
library(metaboData)
library(magrittr)

data('abr1')
```

## Basic Usage

### Analysis data class

*metabolyseR* has three main functions for performing analyses:

* `analysisParameters` - allows the selection of analysis parameters.
* `metabolyse` - input data and info to analyse using the selected parameters.
* `reAnalysis` - re-analyse data previously analysed using `metabolyse` based on newly selected parameters.

#### Parameter Selection
Parameter selection is the fundamental aspect of using *metabolyseR* and will be the most modified as it dictates how the analyses will be performed.
`analysisParameters` returns an object of class `AnalysisParameters` containing the relevant parameters of the selected analysis elements.
For example, the code below will return default parameters for all the *metabolyseR* analysis elements.

```{r parametersExample,eval=FALSE}
library(metabolyseR)
parameters <- analysisParameters()
parameters
```

To retrieve parameters for a subset of elements the following can be run, which will return parameters for only the `preTreat` and   `classification` elements. 

```{r parametersElementSubset,eval=FALSE}
parameters <- analysisParameters(c('preTreat','classification'))
parameters
```

These parameters have been named so that a parameters that denote the same functionality will be named commonly across all methods.
The `changeParameter` function can be used to uniformly change these parameters across all of the selected methods.
The example below changes the defaults of all the parameters named `clusterType` from `FORK` to `PSOCK`.

```{r changeParameter,eval=F}
parameters <- analysisParameters()
parameters
parameters <- changeParameter('cls','day',parameters)
parameters
```

To change parameters individually, the `@` extraction operator can be used to access the relevant slot directly.
For instance, the following code illustrates how to modify the occupancy threshold parameter for class imputation for data pre-treatment from 2/3 to 1/2.

```{r parametersSetExample,eval=FALSE}
parameters@preTreat$impute$class$occupancy <- 1/2
parameters
```

Discussion of parameters particular to the individual analysis elements can be found in the relevant analysis element sections.

Due to the complex nature of the list structures needed for the analyses containing many components, it is possible to express these parameters more simply in YAML format and parse these into R using `parseParameters()`.
Below shows an example `.yaml` file that would give the same default `AnalysisParameters` object when `analysisParameters()` is called.

```{r defaultYAML, eval=FALSE}
library(yaml)
paramFile <- system.file('defaultParameters.yaml',package = 'metabolyseR')

read_yaml(paramFile) %>%
  as.yaml() %>%
  cat()
```

This can be parsed directly into an `AnalysisParameters` object using the following:

```{r parseParametes,eval=F}
p <- parseParameters(paramFile)
p
```

For more complex pre-treatment situations such as the following:

```{r exampleYAML}
library(yaml)
exampleParamFile <- system.file('exampleParameters.yaml',package = 'metabolyseR')

read_yaml(exampleParamFile) %>%
  as.yaml() %>%
  cat()
```

Where multiple remove steps are needed, these are numbered sequentially.
Also where multiple values need to be provided to a particular argument (e.g. `classes = c('H','1')`), these should be supplied as a hyphenated list.

```{r}
p <- parseParameters(exampleParamFile)
p
```

#### Analysis

Executing analyses is simple using the `metabolyse` function with specification of the data to use, the sample meta-information (info) and the analysis parameters.
This is shown by the following code.

```{r metabolyseExample,eval=FALSE}
data(abr1,package = 'FIEmspro')
parameters <- analysisParameters(c('preTreat','classification'))
parameters@preTreat <- list(
    occupancyFilter = list(maximum = list()),
    transform = list(TICnorm = list())
)
analysis <- metabolyse(abr1$neg,abr1$fact,parameters) 
analysis
```

The data and info can be either `tibble` or `data.frame` structures and the parameters should be an S4 object of class `AnalysisParameters`.
The order of the rows of the info table should agree with those in the data table.

`metabolyse` returns an S4 object of class `Analysis`.
Data and analysis results can be extracted from this object using the following functions:

* `rawData()` - raw data.
* `rawInfo()` - raw info.
* `preTreatedData()` - pre-treated data.
* `preTreatedData()` - pre-treated info.
* `classificationResults()` - classification results.
* `featureSelectionResults()` - feature selection results.
* `correlationResults()` - correlation results.

If data pre-treatment has not been performed prior to classification or feature selection, the raw data will automatically be used.
The date and time of each of analysis of each of the elements is also stored and can is displayed when the object is printed.

#### Re-Analysis

There are likely to be occasions where analyses need to be re-analysed using a new set of parameters. 
`reAnalyse` can be used for this.
It simply requires the specification of the `Analysis` object and parameters for the re-analysis as shown below.

```{r reAnalyseExample,eval=FALSE}
parameters <- analysisParameters('featureSelection')
analysis <- reAnalyse(analysis,parameters)
analysis
```

## Pre-Treatment

Data pre-treatment prior to statistical analyses is an essential step in the analysis of metabolomics data as it ensures it's quality and integrity.
It is also important that appropriate pre-treatment strategies are used not only for the analytical technique being applied but are also suitable for the statistical or machine learning analyses that are to be used on the data.

Data pre-treatment is the most faceted aspect of the analysis elements in *metabolyseR*.
It itself is made up of a number of elements, which themselves are made up of methods.
The pre-treatment elements can be seen below.

<center>
```{r preTreatmentDiagram,echo=FALSE,fig.width=8}
DiagrammeR::grViz(readr::read_file('figures/preTreat.gv'))
```
</center>

The parameter selection for the pre-treatment elements will firstly be discussed, with more in-depth discussion on the methods of the individual elements to follow. 

### Parameter Selection

The default pre-treatment parameters can be seen below.

```{r preTreatDefaults}
analysisParameters('preTreat')
```

This pre-treatment analysis is made up of three elements that include variable filtering based on QC samples, missing data imputation and data transformation (`QC`, `impute` and `transform` respectively).
Each of these elements is made up of varying numbers of methods, with each method having its own arguments.
The order in which these elements and their methods is displayed is the order in which their execution upon the data will take place.
This allows the fine specification of the data pre-treatment routines that are required for the particular data.

Pre-treatment routines can be customised by altering the `preTreat` slot within the `AnalysisParameters` object as shown below.

```{r preTreatParameterExample}
data("abr1",package = 'FIEmspro')
parameters <- analysisParameters('preTreat')
parameters@preTreat <- list(
    remove = list(
      class = list(
        cls = 'class',
        classes = unique(abr1$fact$class)[!(unique(abr1$fact$class) %in% c(1:2))])),
    occupancyFilter = list(
      maximum = list()),
    transform = list(
      TICnorm = list())
)
parameters
```

This routine will firstly remove a number of classes, filter the variables based on class occupancy and then transform the data using a total ion count normalisation.

The pre-treatment routines are specified as a hierarchy of lists within lists.
Firstly the pre-treatment elements should be specified within a list with the particular element methods specified as lists within these.
The method arguments are then declared within these lists.
Empty lists for methods can be specified to use the default arguments; however, element lists should not be empty. 

The pre-treatment routine can then be executed using the following.
```{r preTreatExample,eval=F}
analysis <- metabolyse(abr1$neg,abr1$fact,parameters)
analysis
```

### Sample and variable removal - `remove`

The removal of samples prior to their analysis is important if there are outlier samples included or sample classes that are no longer relevant for the intended analyses.

Variable removal is important if for example there is contaminant features present in a data set. 

The methods and their arguments for the removal of samples is shown in the table below.

```{r removeMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::removeMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::removeMethods())
defaults <- lapply(metabolyseR:::removeMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Keep samples and variables - `keep`

The opposite of the remove methods discussed above. 
Useful if there are fewer samples, classes or variables to keep than to remove.

The methods and their arguments for keeping samples is shown in the table below.

```{r keepMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::keepMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::removeMethods())
defaults <- lapply(metabolyseR:::removeMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```


### Batch correction `correction`

There can sometimes be artificial batch related variability introduced introduced into metabolomics analyses as a result of analytical instrumentation or sample preparation.
With appropriate sample randomisation (see section Variable filtering based on QC samples) batch related variability can be corrected for using the methods shown in the table below.

```{r correctionMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::correctionMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::correctionMethods())
defaults <- lapply(metabolyseR:::correctionMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Sample aggregation - `aggregate`

Sample aggregation allows the electronic pooling of samples based on a grouping variable.
This is useful in situations such as the presence of technical replicates that can be aggregated to reduce the effects of pseudo replication.

The methods and their arguments are shown in the table below.

```{r aggregateMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::aggregateMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::aggregateMethods())
defaults <- lapply(metabolyseR:::aggregateMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Variable filtering based on occupancy - `occupancyFilter`

Occupancy provides a useful measure by which to filter variables that are poorly represented (contain a majority of missing values).
An occupancy threshold provides a means of specifying this majority with variables below the threshold excluded from further analyses.
However, this can be complicated by an underlying class structure present within the data where a variable may be well represented within one class but not in another.

An occupancy threshold can be applied both minimally and maximally within the class structure.
A minimally applied threshold would require all the classes to have an occupancy above the threshold for the variable to be retained.
A maximally applied threshold would require only a single class to have an occupancy above the threshold for the variable to be retained.
The later strategy has the important benefit of retaining any presence and absence trends that may be present.

The methods and their arguments for occupancy based variable filtering are shown in the table below.

```{r occupancyMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::occupancyMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::occupancyMethods())
defaults <- lapply(metabolyseR:::occupancyMethods(),formals)
defaults <- lapply(defaults,function(x){x$dat <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Variable filtering based on QC samples - `QC`

A QC sample is an average pooled sample, equally representative in composition of all the samples present within an experimental set.
Within an analytical run, the QC sample is analysed intermittently at equal intervals throughout the run.
If there is class structure within the run this should be randomised within a block fashion so that the classes are equally represented in each block throughout the run.
QC samples can then be injected between these randomised blocks.
This provides a set of technical injections that allows the variability in instrument performance over the run to be accounted for and the robustness of the acquired variables to be assessed.

The technical reproducibility of an acquired variable can be assessed using it's relative standard deviation (RSD) within the QC samples.
The variable RSDs can then be thresholded to filter out variables that are poorly reproducible across the analytical runs.
This variable filtering strategy has an advantage over that of occupancy alone as it is not dependent on underlying class structure.
Therefore the variables and variable numbers will not alter if a new class structure is imposed upon the data.

The methods and arguments for variable filtering based upon QC samples are shown in the table below.

```{r QCMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::QCMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::QCMethods())
defaults <- lapply(metabolyseR:::QCMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Imputation of missing data - `impute`

Missing values can have an important influence on downstream analyses with zero values heavily influencing the outcomes of parametric tests.
Where and how they are imputed are important considerations and is highly related to variable occupancy.
Imputation accuracy is likely to be reduced if data is sparse or there is underlying class structure where there is significant discrimination.

The methods provided here allow both these aspects to be taken into account and utilise Random Forest imputation using the [*missForest*]( https://CRAN.R-project.org/package=missForest) package.
Their arguments are shown in the table below.

```{r imputeMethodsTable,echo=FALSE,results='asis'}
desc <- metabolyseR:::imputeMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::imputeMethods())
defaults <- lapply(metabolyseR:::imputeMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})
if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

### Data transformation - `transform`

Prior to downstream analyses, metabolomics data often require a transformation step to allow the data to meet the assumptions of a particular statistical inference or account for potential variability in sample concentration.

There are a wide range of transformation methods available that are commonly used for the analysis of metabolomics data.
The methods and their arguments are shown in the table below.

```{r transformMethods,echo=FALSE,results='asis'}
desc <- metabolyseR:::transformMethods(description = T)
args <- lapply(desc,function(x){return(x$arguments)})
args <- lapply(args,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
args <- lapply(args,function(x){apply(x,1,function(y){paste(y,collapse = ' - ')})})
if (T %in% (sapply(args,length) > 1)) {
  args[sapply(args,length) > 1] <- lapply(args[sapply(args,length) > 1],function(x){paste(x,collapse = '; ')})
}
args <- unlist(args)
if (T %in% sapply(args,function(x){grepl("''",x)})) {
  args[sapply(args,function(x){grepl("''",x)})] <- lapply(args[sapply(args,function(x){grepl("''",x)})],function(x){''})
}
desc <- unlist(lapply(desc,function(x){return(x$description)}))
meth <- names(metabolyseR:::transformMethods())
defaults <- lapply(metabolyseR:::transformMethods(),formals)
defaults <- lapply(defaults,function(x){x$d <- NULL;return(x)})
if (T %in% sapply(defaults,is.null)) {
  defaults[sapply(defaults,is.null)] <- lapply(defaults[sapply(defaults,is.null)],function(x){c(`''` = '')})
}
defaults <- lapply(defaults,function(x){tibble::tibble(Parameter = names(x),Value = unlist(x))})
defaults <- lapply(defaults,function(x){apply(x,1,function(y){paste(y,collapse = ' = ')})})

if (T %in% sapply(defaults,function(x){grepl("''",x)})) {
  defaults[sapply(defaults,function(x){grepl("''",x)})] <- lapply(defaults[sapply(defaults,function(x){grepl("''",x)})],function(x){''})
}

if (T %in% (sapply(defaults,length) > 1)) {
  defaults[sapply(defaults,length) > 1] <- lapply(defaults[sapply(defaults,length) > 1],function(x){paste(x,collapse = '; ')})
}
defaults <- unlist(defaults)
tab <- tibble::tibble(Method = meth, Description = desc, Arguments = args, Defaults = defaults)

knitr::kable(tab)
```

## Modelling

Modelling provides the essential data mining step for extracting biological information from a data set relating to experimental conditions.
`metabolyseR` provides a number of both univariate and multivariate methods for data mining.

The default parameters for modelling using Random Forest are shown below.

```{r modellingParameters}
parameters <- analysisParameters('modelling')
parameters
```

Parameters for alternative methods or multiple methods can be retreived using the `modellingParameters()` function as shown below.

```{r changeModellingParameters}
parameters@modelling <- modellingParameters(c('randomForest','ttest'))
parameters
```

Descriptions of each of the parameters can be found in the documentation for a particular method (see table below). 

Modelling analysis can then be performed on the pre-treated data from the previous section.

```{r classificationAnalysis,eval=FALSE}
analysis <- reAnalyse(analysis,parameters)
```

The modelling results will then be returned as a list within the `Analysis` object containing the results object for each method.

The table below outlines the modelling methods currently available:

```{r modellingMethods,echo=FALSE,results='asis'}
metabolyseR:::modellingMethods(description = T) %>%
  purrr::map(tibble::as_tibble) %>%
  dplyr::bind_rows(.id = 'Method') %>%
  {set_colnames(.,stringr::str_to_title(colnames(.)))} %>%
  knitr::kable()
```

## Correlations

Correlations are important for finding associations between metabolomic features.
They can be indicative of isotopic, adduct and bio-transformation relationships that can aid their annotation.

The default parameters for correlation analysis are shown below.

```{r correlationsParameters}
parameters <- analysisParameters('correlations')
parameters
```

These parameters denote the following:

* `method` - Correlation type (`?Hmisc::rcorr`).
* `pAdjustMethod` - Method for multiple testing p value correction (`?p.adjust`).
* `corPvalue` - Threshold for *p* value significance.
* `nCores` - The number of cores to use for parallelisation.
* `clusterType` - The cluster type to use for parallelisation (`?parallel::makeCluster`).

This correlation analysis can then be performed using the following.

```{r correlationsExample,eval=FALSE}
analysis <- reAnalyse(analysis,parameters)
```

The results will be returned as a table containing the significantly correlated bins, their correlation coefficients and their log~2~ intensity ratios.

## Visualisation

*metabolyseR* provides a number of functions for the visualisation of analysis results:

* quality control
    + `plotTIC()` - total ion counts of sample raw data
    + `plotRSD()` - relative standard deviation distributions of quality control samples
    + `plotPCA()` - principle component analysis results of pre-treated data
    + `plotLDA()` - linear discriminant analysis results of pre-treated data
    + `plotUnsupervisedRF()` - MDS of unsupervised random forest proximities of pre-treated data  
    + `plotSupervisedRF()` - MDS of supervised random forest proximities of pre-treated data
    
* modelling
    + `plotExplanatoryHeatmap()` - heat map of explanatory features
    + `plotFeature()` - feature trends

## Workflow Integration

The generic nature of *metabolyseR* means that it can easily be integrated into workflows for the analysis of any metabolomics data.
The package [*metaboWorkflows*](https://github.com/jasenfinch/metaboWorkflows) provides wrapper methods for signal processing, analysis and annotation of metabolomics data.
The *metabolyseR* analysis elements are incorporated into the analysis aspect of this package.